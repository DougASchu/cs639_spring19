{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Store's Exploratory data analysis(EDA) With Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Exploratory Data Analysis\n",
    "In statistics, exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.\n",
    "\n",
    "This is an example of EDA in Python from [here](https://medium.com/python-pandemonium/introduction-to-exploratory-data-analysis-in-python-8b6bcb55c190)! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA with Python\n",
    "\n",
    "Multiple libraries are available to perform basic EDA but here we will use [Pandas](https://pandas.pydata.org) and [matplotlib](https://matplotlib.org) for this post. We will use pandas for data manipulation and matplotlib for plotting graphs. We are using a Jupyter notebook to write code and showcase our graphs.\n",
    "\n",
    "Links:\n",
    "* 10 minutes to Pandas: [https://pandas.pydata.org/pandas-docs/stable/10min.html](https://pandas.pydata.org/pandas-docs/stable/10min.html)\n",
    "* 20 minutes to Matplotlib [https://www.tutorialdocs.com/article/python-matplotlib-tutorial.html](https://www.tutorialdocs.com/article/python-matplotlib-tutorial.html]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Let's get Coding!\n",
    "\n",
    "### Step 1: Import Libraries\n",
    "\n",
    "Let's load the necessary libraries in python! You will also notice a special command for the Jupyter notebook `%matplotlib inline`. This command allows you to render plot inline in notebook cells. You will shortly see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load and clean your data\n",
    "\n",
    "First, we read the data for the appropriate file. The file is located in the same directory as our notebook. If it was located to a different directory we should have specified the appropriate path. \n",
    "\n",
    "The two next commands load the data from the `csv` file and print the first few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Amount` column contains entries with the `$` sign. These entries are interpreted as strings and we cannot run analytics. Let's remove them. Pandas allow you to perform simple string operations. \n",
    "\n",
    "Below, we remove the `$` sign and then convert the string field into numeric. Once done, column `Amount` contains `float` data entries and now we can perform mathematical operations on this field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Amount'] = df['Amount'].str.replace('$','').str.replace(',','')\n",
    "df['Amount'] = pd.to_numeric(df['Amount'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing, the `BranchName` field is unnecessary for our analysis since we only have data from a single store so let’s remove it! Option `inpace=True` removes the field from the existing `DataFrame` without re-assigning it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove unwanted \n",
    "df.drop('BranchName',axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Exploring the clean data\n",
    "Operation cleanup is done, let’s dive into the data and find insights!\n",
    "The very first thing we are going to do is to find out the number of records and the number of columns. For that we will execute the command `df.shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it mean? It’s actually rows * columns. So here there are 4100 records and 9 columns. What if we need a detailed summary of this data? What if we want some statistics? We can use `df.describe()` for this!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some interesting information given here. Let's focus on **count**. We see the same record count 4100 for all columns. This means that there are **no missing fields** in this dataset. You can also check an individual column count, say, for `Units` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Units'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Data Distributions\n",
    "Command `describe` gives us a generic picture: what is the mean, min, and max along with standard deviation (std) and median. The percentiles are also there. Standard Deviation is quite useful tool to figure out how the data is spread above or below the mean. For instance the standard deviation for `Amount` is 183.5, while the mean is around 35. On other hand the mean for `Units` is 12.7 and the standard deviation is 17.85. Let’s plot the empirical distribution of `Amount` column (i.e., a histogram)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_bins = 10\n",
    "plt.hist(df['Amount'], num_bins, normed=1, facecolor='blue', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ignore this spike for a while and notice the base line which is very large! It varies from `-1000` to `1000+`. Let's investigate something more interesting. Let’s find out about the `Sales` by `Month`, `Day`, and `Hour`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Sales by Month, Day and Hour\n",
    "\n",
    "We will use the `.groupby()` functionality of pandas and `.size()` to get the aggregated value of the particular column only. If we want the aggregated value for every column we can use `.count()`. Since we only need for `Month` we use `.size()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_by_month = df.groupby('Month').size()\n",
    "print(sales_by_month)\n",
    "#Plotting the Graph\n",
    "plot_by_month = sales_by_month.plot(title='Monthly Sales',xticks=(1,2,3,4,5,6,7,8,9,10,11,12))\n",
    "plot_by_month.set_xlabel('Months')\n",
    "plot_by_month.set_ylabel('Total Sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that everything was great until July but then something happened and there was a sharp decline in August, then staff tried hard for next 3 months and then things died again. Let's investigate by day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_by_day = df.groupby('Day').size()\n",
    "plot_by_day = sales_by_day.plot(title='Daily Sales',xticks=(range(1,31)),rot=55)\n",
    "plot_by_day.set_xlabel('Day')\n",
    "plot_by_day.set_ylabel('Total Sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sales drop towards the end of each month. We also see that the 18th day is quite good. That day we sold 151 units. What happens every hour?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_by_hour = df.groupby('Hour').size()\n",
    "plot_by_hour = sales_by_hour.plot(title='Hourly Sales',xticks=(range(5,22)))\n",
    "plot_by_hour.set_xlabel('Working Hours')\n",
    "plot_by_hour.set_ylabel('Total Sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Majority of the customers come in afternoon. The frequency gets quite low during closing time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
